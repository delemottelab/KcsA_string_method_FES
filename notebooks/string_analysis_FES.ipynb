{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32867ee4-ea45-4aa9-b192-176b0801c41c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# String Method Analysis Markov-State-Models\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1c221-b47d-4484-9125-6eac0a75a249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "\n",
    "logging.getLogger(\"stringmethod\").setLevel(logging.ERROR)\n",
    "sys.path.append(\"../string-method-gmxapi/\")\n",
    "import src.analysis as spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a829a2-f0ff-42c3-a3be-ec5f9923d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7af371-cecb-41f7-b46d-77e31ddb70cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249545e-242c-4b87-b632-6268e4fe55aa",
   "metadata": {},
   "source": [
    "This notebook needs to run in the string simulation folder, this cell will get you there. You also set up a path for writing the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce510b6f-1e1c-416f-b30b-6bbda63671a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_sim = \"C2I_lb_v1/\"\n",
    "path_raw = f\"/data/sperez/Projects/string_sims/data/raw/{name_sim}\"\n",
    "path_interim = f\"/data/sperez/Projects/string_sims/data/interim/{name_sim}\"\n",
    "path_processed = f\"/data/sperez/Projects/string_sims/data/processed/{name_sim}\"\n",
    "path_report = f\"/data/sperez/Projects/string_sims/reports/figures/{name_sim}\"\n",
    "os.chdir(path_raw)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb2261-372f-460c-a3db-c958e6114a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cv.pkl\", \"rb\") as file:\n",
    "    cvs, ndx_groups = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadba22-5407-4d9c-a90c-b3ed0888f37b",
   "metadata": {},
   "source": [
    "The `load_swarm_data` function will load the swarm data in the `cv_coordinates`. If you set `extract=True` it will read the data from the swarm files. If you have done this previously you can set `extract=False` so the function just reads `postprocessing/cv_coordinates.npy`. `first_iteration` can be used to exclude initial swarms as equilibration and `last_iteration` can be done to exclude some iterations for example if you want to estimate the FES convergence by comparing blocks of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ddb40f-95ec-41d2-a7ec-5df6906f1e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract = True\n",
    "if extract:\n",
    "    cv_coordinates = spc.load_swarm_data(\n",
    "        extract=extract, first_iteration=100, last_iteration=None\n",
    "    )\n",
    "    np.save(f\"{path_interim}cv_coordinates.npy\", cv_coordinates)\n",
    "else:\n",
    "    cv_coordinates = np.load(f\"{path_interim}cv_coordinates.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92260863-37d2-4333-8330-31993199d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = spc.natural_sort(glob.glob(\"./strings/string[0-9]*txt\"))\n",
    "strings = np.array([np.loadtxt(file).T for file in files])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e8384-096e-4878-a796-c1cfb1ce7f3f",
   "metadata": {},
   "source": [
    "# MSM modelling of free energy surface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec4c2e-b848-407d-a946-79850a81368f",
   "metadata": {},
   "source": [
    "## Dimensionality reduction with TICA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17c84c-4f42-4e4f-a944-4ad1b6226251",
   "metadata": {},
   "source": [
    "The following cell computes the tica projection of the string cvs and discards the tics that have the lowest kinetic variance. This reduces the cvs space to a lower dimensional space that is adapted to the kinetic variance. You can use the drop keyword to drop certain cvs that are not well converged in the string simulation or that change very little from the beggining to the end of the string. The best case scenario is that `drop=[]` just works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04164fe3-93ab-4228-a4c4-467efe9ecbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tica = spc.cvs_to_tica(cv_coordinates, drop=[20, 21, 22, 23, 32, 33, 34, 35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e0b54f-a57e-4cf9-8f7d-15a9123e2984",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde8c4f-47c5-4c5f-89d9-1ec645366c6b",
   "metadata": {},
   "source": [
    "The next cell plots the \"vamp score\" of using `n_clustercenters` to make an MSM. You should find that at some point the vamp score saturates. Choose the minimum number of clusters that gives you the saturated vamp score as the value of k for the next steps. This might take a little while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e6526-2107-4133-83ae-47fe3c85c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clustercenters = [5, 10, 30, 50, 75, 100, 200, 500][::-1]\n",
    "fig, ax, vamp_scores = spc.get_vamp_vs_k(n_clustercenters, tica, n_jobs=4)  # 6 min\n",
    "np.save(f\"{path_interim}vamp_scores.npy\", vamp_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eab2eb-06eb-4b71-a75f-7d0aa8221de6",
   "metadata": {},
   "source": [
    "If the calculation fails, there is something wrong with your MSM. Either you have too little transitions or there too many cvs in tica to have all the states well connected. Solutions:\n",
    "+ Reduce the maximum number of clusters (drop 200 and 500) of `n_clustercenters` and see if you get a saturated curve.\n",
    "+ Reduce the number of cvs that went into your TICA calculation.\n",
    "+ Do more iterations of the string method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a82df-a07a-4391-ba1e-a320c10b7a05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MSM Deeptime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88b3d8-10e5-4e85-ad92-0e107eb240c2",
   "metadata": {},
   "source": [
    "Choose the number of clusters, `k`, for the clustering from the previous calculation. Also change n_proc to however many processors you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7701fadd-4a0e-49a9-a6e0-258010e1e54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "clusters = spc.k_means_cluster(tica, k, stride=1, max_iter=500, n_jobs=4, seed=28101990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fcd39-9913-4f1c-8740-2615f430bca6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "msm, weights = spc.get_msm(clusters, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1cd948-f7a4-4f4e-a2ca-edde674518d0",
   "metadata": {},
   "source": [
    "# FES projection on IG vs SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3673fc24-a1dd-4577-9ed1-70d614e09bd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CVs for projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc2733a-eb34-483c-874c-905e45d9b559",
   "metadata": {},
   "source": [
    "Make a `cv_proj` numpy array with shape (n_iteration * n_swarms_iterations, n_frames_per_iter, 2). n_frames_per_iter is usally 2 since you only record the value of the cvs at the begining and end of the swarm. The last dimesions are the cvs on which you would like to project your FES using the weights obtained from the msm. The FES is then the negative log of a *weighted* histogram of the projection cvs using the weights from the msm. The projection cvs can be anything that you can calculate for a structure, not necessarily the cvs of the string. In the example bellow it is the mean of two cvs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d189b7-5125-41fd-8d04-a12a11a5bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_proj = spc.cvs_to_SF_IG(cv_coordinates, [0, 1], [10, 11])\n",
    "np.save(f\"{path_interim}cv_proj.npy\", cv_proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa20877-6de5-4e29-8d35-66b257239baf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project FES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c16874-f139-43e0-a169-6028ea3ec2f1",
   "metadata": {},
   "source": [
    "Do the projection and take log. You have to choose a bandwidth for the [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) of the histogram. It should be big enough to reduce noise but not so big to remove features. If you give `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cc621-7b9d-4c83-8f1b-7f0b2a8eea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bandwidth = 0.05\n",
    "p_of_cv, extent, _ = spc.get_kde(cv_proj, weights, bandwidth)\n",
    "F0 = -np.log(p_of_cv)\n",
    "F = F0 - F0.min()\n",
    "F[F > 40] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec818b9-9192-42f6-afd2-06cdcd14fe09",
   "metadata": {},
   "source": [
    "Do the projection and take log. You have to choose a bandwidth for the [KDE](https://en.wikipedia.org/wiki/Kernel_density_estimation) of the histogram. It should be big enough to reduce noise but not so big to remove features. If you give `None`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0826f1ce-e5e4-4683-9575-2560378eabd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"{path_processed}FES_SF_IG.npy\", F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb586f-ebb8-4c91-be92-a09802b5e2bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot FES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed42a66-3599-411b-af26-88990691e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = spc.plot_2D_heatmap(\n",
    "    F,\n",
    "    extent,\n",
    "    f_max=25,\n",
    "    f_min=0,\n",
    "    cbar_label=\"Free Energy (kT)\",\n",
    "    xlabel=\"SF (nm)\",\n",
    "    ylabel=\"IG (nm)\",\n",
    ")\n",
    "ax.set_xlim([0.48, 1.0])\n",
    "ax.set_ylim([1.1, 2.45])\n",
    "fig.tight_layout()\n",
    "fig.savefig(path_report + \"FES.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4af9d6-c73a-4b44-b786-a9ec31571f46",
   "metadata": {},
   "source": [
    "## Bootstrap to get error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaeb808-1306-4705-9bec-e225d9ecf993",
   "metadata": {},
   "source": [
    "The problem with calculating errors in MD is that most statistical techniques for this rely on the data being uncorrelated. MD data is most of the time highly correlated due to the proximity in time and starting structure. Correlated data generates artificially low error estimates. \n",
    "\n",
    "For this reason we use blocking. In our case we will use blocking+bootstrapping. This is very well explained in this [very usefull video](https://www.youtube.com/watch?v=gHXXGYIgasE&t=1854s) by prof. Giovanni Bussi.\n",
    "\n",
    "The uncertainty is calculated as half of the interval containing 95% of the probability of the distribution of histograms generated in the bootstraps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62641e3-4288-47b8-95f3-04a15f1b1ecc",
   "metadata": {},
   "source": [
    "This part is probably going to be slow! Maybe it will go over night. It is actually doing len(blocks) * n_boot msms! The good things is that once you have figured out for your system (and similar systems) what is a reasonable number of blocks then you can just do `blocks=[my_reasonable_number_blocks]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d36c9e-167b-403b-add3-8dffd1fc455d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import src.analysis as spc\n",
    "\n",
    "blocks = [2, 4, 8, 16, 32]\n",
    "n_blocks = len(blocks)\n",
    "n_boot = 100\n",
    "calculate = False\n",
    "if calculate:\n",
    "    errors, his, hdis = spc.get_error(\n",
    "        cv_proj,\n",
    "        clusters,\n",
    "        extent,\n",
    "        n_boot=n_boot,\n",
    "        bandwidth=bandwidth,\n",
    "        nbin=55,\n",
    "        blocks=blocks,\n",
    "        seed=28101990,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    np.save(f\"{path_processed}errors_{n_boot}_{n_blocks}.npy\", errors)\n",
    "else:\n",
    "    errors = np.load(f\"{path_processed}errors_{n_boot}_{n_blocks}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85dd56-f5c9-4a95-93ad-d1fcd8e3d4d5",
   "metadata": {},
   "source": [
    "Choose the number of blocks that gives you a high error.\n",
    "\n",
    "Note,`e_min` and `e_max` are choosen to remove the extremely high or low values of error that are generated due to poor sampling or high free energy. These regions of the \"free error surface\" are not what we care about and thus we remove it from the statistic and the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce89387-9b14-4e7d-bbf4-0c63f3f32abb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "for error in glob.glob(path_processed + \"errors_*.npy\"):\n",
    "    errors = np.load(error)\n",
    "    e_max = 6\n",
    "    e_min = 1.0e-03\n",
    "    e = errors.copy()\n",
    "    e[e > e_max] = np.nan\n",
    "    e[e <= e_min] = np.nan\n",
    "    label = f\"n_boot={error.split('/')[-1].split('_')[1]}\"\n",
    "    mean = np.nanmean(e, axis=(1, 2))\n",
    "    std_err = np.nanstd(e, axis=(1, 2)) / np.sqrt(e.shape[0])\n",
    "    ax.plot(np.array(blocks), mean, marker=\"o\", label=label)\n",
    "    ax.fill_between(np.array(blocks), mean + std_err, mean - std_err, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of blocks\", size=15)\n",
    "ax.set_ylabel(\"FES error (kT)\", size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdea64c-85e5-475a-869b-53ab8c087536",
   "metadata": {},
   "source": [
    "From the previous plot you can see which is the adequate number of blocks that low but still gives you the plateauing (or highest) error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718d087-a494-4ab4-8cd9-6b212685982b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "number_blocks = 8\n",
    "n_boot = 100\n",
    "n_blocks = len(blocks)\n",
    "errors = np.load(f\"{path_processed}errors_{n_boot}_{n_blocks}.npy\")\n",
    "f_max = 20\n",
    "e_max = 6\n",
    "e_min = 1.0e-03\n",
    "\n",
    "e = errors[blocks.index(number_blocks)].copy()\n",
    "e[e > e_max] = np.nan\n",
    "e[e <= e_min] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10 * 2, 7), sharex=True, sharey=True)\n",
    "_ = spc.plot_2D_heatmap(\n",
    "    F,\n",
    "    extent,\n",
    "    f_max=25,\n",
    "    f_min=0,\n",
    "    cbar_label=\"Free Energy (kT)\",\n",
    "    xlabel=\"SF (nm)\",\n",
    "    ylabel=\"IG (nm)\",\n",
    "    fig=fig,\n",
    "    ax=ax[0],\n",
    ")\n",
    "_ = spc.plot_2D_heatmap(\n",
    "    e,\n",
    "    extent,\n",
    "    f_max=e_max,\n",
    "    f_min=0,\n",
    "    cbar_label=\"FES Uncertainty (kT)\",\n",
    "    xlabel=\"SF (nm)\",\n",
    "    cmap=plt.cm.viridis_r,\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    ")\n",
    "ax[1].set_title(\"Bootstrap Error (95%)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(path_report + \"FES_error.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76920fc4-2f0a-4715-b002-1fce0a9ac28f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Path CVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe8b3aa-d5fd-4d8e-a5b9-a9968a96b88f",
   "metadata": {},
   "source": [
    "Define path as the average of the last av_last_n_it of the strings. Obtain a reasonable guess of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd80863-6871-472d-a54d-f78667094661",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_strings = strings.shape[0]\n",
    "av_last_n_it = 25\n",
    "path = np.mean(strings[n_strings - av_last_n_it :, :, :], axis=0)\n",
    "lam = spc.get_path_lambda(path)\n",
    "print(f\"Lambda value for path {lam:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39584f-af26-4076-bb62-77fe5e76963a",
   "metadata": {},
   "source": [
    "Let's see if the lambda gives a well behaved path cv. The progress variable (s) should be increasing in the path itself and the distance to path variable (z) be low and constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee9208-2669-4664-9e62-1f6c50684083",
   "metadata": {},
   "source": [
    "## Representation of Path CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6eb41e-8096-48ee-9338-29f07e301c89",
   "metadata": {},
   "source": [
    "### Load data and calculate path cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58565b-2c26-4159-8015-0a6c0029ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_of_path = []\n",
    "for p in path.T:\n",
    "    cv_of_path.append(spc.cvs_to_path(p, path=path, lam=lam))\n",
    "cv_of_path = np.array(cv_of_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f547fff7-7642-460b-a5db-43e23f381dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_path = []\n",
    "for i in range(cv_coordinates.shape[0]):\n",
    "    cvs_path.append([])\n",
    "    for j in range(cv_coordinates.shape[1]):\n",
    "        cvs_path[i].append(spc.cvs_to_path(cv_coordinates[i, j, :], path=path, lam=lam))\n",
    "cvs_path = np.array(cvs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98c486-604c-4770-962b-18f7b20eca53",
   "metadata": {},
   "source": [
    "### Path CV on final string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb600e-d7e1-4eea-8de9-0d7626a382f3",
   "metadata": {},
   "source": [
    "Check how the path cv is represented in the average final string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25045bd4-b817-4ac4-80b8-9429b3ececf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10 * 2, 7))\n",
    "ax[0].plot(cv_of_path[:, 0], marker=\"o\")\n",
    "ax[1].plot(cv_of_path[:, 1], marker=\"o\")\n",
    "ax[0].set_xlabel(\"bead number\")\n",
    "ax[0].set_ylabel(\"S\")\n",
    "ax[0].set_xlabel(\"bead number\")\n",
    "ax[0].set_ylabel(\"Z\")\n",
    "ax[0].set_title(f\"Path-variable {lam = :.2f}\")\n",
    "ax[1].set_title(f\"Path-variable {lam = :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc20b2-ce01-4cc4-8e2b-03d8ba2fa98d",
   "metadata": {},
   "source": [
    "### Path CV projected on IG vs SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298b980-0a27-426a-a1a5-b0ac0ec4e720",
   "metadata": {},
   "source": [
    "Check how the path variables project onto the canonical inactivation 2CV FES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87a7e8-7da7-4b9b-9b1a-3c7fac815f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwidth = None\n",
    "density_cut_off = -0.00001\n",
    "print(f\"If {density_cut_off:= e} we have F={-np.log(density_cut_off):= e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e8900-abb7-4387-a630-c5526d249dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_of_cv, extent = spc.project_property_on_cv_kde(\n",
    "    cv_proj,\n",
    "    weights=weights,\n",
    "    proper=cvs_path[:, :, 0:1],\n",
    "    bandwidth=bandwidth,\n",
    "    density_cut_off=density_cut_off,\n",
    ")\n",
    "np.save(f\"{path_processed}s_path_of_SF_IG.npy\", s_of_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c088f-e5a7-40c6-86c9-035f82b522eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_of_cv, extent = spc.project_property_on_cv_kde(\n",
    "    cv_proj,\n",
    "    weights=weights,\n",
    "    proper=cvs_path[:, :, 1:2],\n",
    "    bandwidth=bandwidth,\n",
    "    density_cut_off=density_cut_off,\n",
    ")\n",
    "np.save(f\"{path_processed}z_path_of_SF_IG.npy\", z_of_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe4e45d-b83e-4633-a353-16aa71e7f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_of_cv[s_of_cv > 2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d0355-e9b4-44ce-965c-c6f53cda478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_of_cv[z_of_cv > 2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088e25a-3cb9-429f-8da8-053faa178d86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10 * 2, 7), sharex=True, sharey=True)\n",
    "_ = spc.plot_2D_heatmap(\n",
    "    s_of_cv,\n",
    "    extent,\n",
    "    cbar_label=\"s[path]\",\n",
    "    xlabel=\"SF (nm)\",\n",
    "    ylabel=\"IG (nm)\",\n",
    "    f_min=0,\n",
    "    f_max=1,\n",
    "    fig=fig,\n",
    "    cmap=plt.cm.Spectral,\n",
    "    ax=ax[0],\n",
    "    n_colors=200,\n",
    ")\n",
    "ax[0].contour(F, levels=20, extent=extent, vmin=0, vmax=20, colors=\"k\")\n",
    "ax[0].grid(None)\n",
    "_ = spc.plot_2D_heatmap(\n",
    "    z_of_cv,\n",
    "    extent,\n",
    "    cbar_label=\"z[path]\",\n",
    "    xlabel=\"SF (nm)\",\n",
    "    cmap=plt.cm.magma,\n",
    "    f_min=0,\n",
    "    f_max=0.4,\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    "    n_colors=200,\n",
    ")\n",
    "ax[1].contour(F, levels=20, extent=extent, vmin=0, vmax=20, colors=\"w\")\n",
    "ax[1].grid(None)\n",
    "fig.tight_layout()\n",
    "fig.savefig(path_report + \"projection_path_cv.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd700155-1b0c-47a7-84b8-935e7083c273",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate FES projected on path CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd299e96-07f2-4ed4-8b3f-01d9524b335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.analysis as spc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1c78db-67c6-4134-9f4e-17b2db680540",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_path = cvs_path[:, :, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16995768-e429-416f-8f29-6f301b7c9165",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bandwidth = 0.25\n",
    "nbins = 55\n",
    "p_of_cv, extent, kernel = spc.get_kde(s_path, weights, bandwidth, nbins=nbins)\n",
    "F0 = -np.log(p_of_cv)\n",
    "F = F0 - F0.min()\n",
    "F[F > 40] = np.nan\n",
    "s = np.linspace(extent[0], extent[1], nbins)\n",
    "np.save(f\"{path_processed}FES_path.npy\",np.vstack([s, F]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718e5f5-5f84-4754-86b6-38cc55ae2d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import src.analysis as spc\n",
    "\n",
    "blocks = [2, 4, 8, 16, 32]\n",
    "n_blocks = len(blocks)\n",
    "n_boot = 100\n",
    "calculate = False\n",
    "if calculate:\n",
    "    errors, his, hdis = spc.get_error(\n",
    "        s_path,\n",
    "        clusters,\n",
    "        extent,\n",
    "        n_boot=n_boot,\n",
    "        bandwidth=bandwidth,\n",
    "        nbin=nbins,\n",
    "        blocks=blocks,\n",
    "        seed=28101990,\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    np.save(f\"{path_processed}path_errors_{n_boot}_{n_blocks}.npy\", errors)\n",
    "    np.save(f\"{path_processed}path_hdis_{n_boot}_{n_blocks}.npy\", hdis)\n",
    "else:\n",
    "    errors = np.load(f\"{path_processed}path_errors_{n_boot}_{n_blocks}.npy\")\n",
    "    hdis = np.load(f\"{path_processed}path_hdis_{n_boot}_{n_blocks}.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8291d-3220-4626-a2f3-becbb87c57e6",
   "metadata": {},
   "source": [
    "Choose the number of blocks that gives you a high error.\n",
    "\n",
    "Note,`e_min` and `e_max` are choosen to remove the extremely high or low values of error that are generated due to poor sampling or high free energy. These regions of the \"free error surface\" are not what we care about and thus we remove it from the statistic and the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7b70ac-a9d5-4069-9f91-3b6b913f4885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "for error in glob.glob(path_processed + \"path_errors_*.npy\"):\n",
    "    errors = np.load(error)\n",
    "    e_max = 6\n",
    "    e_min = 1.0e-03\n",
    "    e = errors.copy()\n",
    "    e[e > e_max] = np.nan\n",
    "    e[e <= e_min] = np.nan\n",
    "    label = f\"n_boot={error.split('/')[-1].split('_')[2]}\"\n",
    "    mean = np.nanmean(e, axis=1)\n",
    "    std_err = np.nanstd(e, axis=1) / np.sqrt(e.shape[0])\n",
    "    ax.plot(np.array(blocks), mean, marker=\"o\", label=label)\n",
    "    ax.fill_between(np.array(blocks), mean + std_err, mean - std_err, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Number of blocks\", size=15)\n",
    "ax.set_ylabel(\"FES error (kT)\", size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcb6a6-357a-418e-bf5e-c611af73b891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_block = 16\n",
    "n_boot = 50\n",
    "n_blocks = len(blocks)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "hdis = np.load(f\"{path_processed}path_hdis_{n_boot}_{n_blocks}.npy\")\n",
    "low = hdis[blocks.index(error_block), 0, :]\n",
    "high = hdis[blocks.index(error_block), 1, :]\n",
    "ax.plot(s, F, label=label)\n",
    "ax.fill_between(s, high, low, alpha=0.3)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"s[path]\", size=15)\n",
    "ax.set_ylabel(\"F (kT)\", size=15)\n",
    "fig.savefig(path_report + \"FES_path_cv.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf5a472-ff2a-4806-a3ec-43a7c8f9776e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "string_method",
   "language": "python",
   "name": "string_method"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
